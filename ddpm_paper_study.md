Denoising Diffusion Probabilistic Models (DDPM)

Jonathan Ho, Ajay Jain, Pieter Abbeel (UC Berkeley), NeurIPS 2020
arXiv:2006.11239

â¸»

ğŸ“Œ 1. í•µì‹¬ ì•„ì´ë””ì–´ ìš”ì•½
	â€¢	DDPMì€ Markov Chain ê¸°ë°˜ latent variable modelë¡œ, ë°ì´í„°ë¥¼ ì ì°¨ì ìœ¼ë¡œ ë…¸ì´ì¦ˆí™”í•œ í›„ ì—­ë°©í–¥ìœ¼ë¡œ ë³µì›í•˜ëŠ” ë°©ì‹.
	â€¢	Forward Process: Gaussian noiseë¥¼ ì ì§„ì ìœ¼ë¡œ ì¶”ê°€.
	â€¢	Reverse Process: í•™ìŠµëœ neural network (U-Net ê¸°ë°˜)ê°€ í•´ë‹¹ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•´ ì›ë³¸ì„ ë³µì›.
	â€¢	Training ObjectiveëŠ” variational boundë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ì´ ê³¼ì •ì—ì„œ denoising score matchingê³¼ Langevin dynamicsê°€ ì—°ê²°ë¨.

ğŸ“ DDPMì´ latent variable modelì¸ ì´ìœ 

â€œì§„ì§œ latent variable modelì¸ê°€?â€ ë¼ëŠ” ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ì„  êµ¬ì¡°ì™€ ìˆ˜ì‹ ê¸°ë°˜ í•´ì„ì´ í•„ìš”í•¨.

	â€¢	ì¼ë°˜ì ì¸ latent variable modelì€ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¥¼ ê°€ì§:
$$ p(x) = \int p(x|z)p(z)dz $$
	â€¢	DDPMë„ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ë¨:
$$ p_\theta(x_0) = \int p_\theta(x_{0:T}) dx_{1:T} $$
	â€¢	ì—¬ê¸°ì„œ $x_1, â€¦, x_T$ëŠ” dataì™€ ë™ì¼ ì°¨ì›ì˜ ì—°ì† latentë¡œ í•´ì„ë¨ â†’ trajectory ê¸°ë°˜ latent structure
	â€¢	ForwardëŠ” encoder ì—­í•  (ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€), ReverseëŠ” decoder ì—­í•  (ë³µì›)
	â€¢	$x_T \sim \mathcal{N}(0, I)$ ëŠ” prior ì—­í• ì„ ìˆ˜í–‰

ğŸ§© ê²°êµ­ DDPMì€:
	â€¢	latentëŠ” í•˜ë‚˜ì˜ ë²¡í„°ê°€ ì•„ë‹ˆë¼ ì—°ì†ì ì¸ ë…¸ì´ì¦ˆ ìƒíƒœë“¤ $x_{1:T}$
	â€¢	ì´ë“¤ì„ í†µí•´ $x_0$ì„ ìƒì„±í•˜ë©°, variational inferenceë¥¼ í†µí•œ í•™ìŠµì„ ìˆ˜í–‰
	â€¢	ë”°ë¼ì„œ ëª…ë°±í•œ latent variable modelì˜ êµ¬ì¡°ë¥¼ ë”°ë¦„

â¸»

ğŸ§  2. êµ¬ì¡° ê°œìš” (Forward & Reverse Process)

2.1 Forward process (Diffusion)
	â€¢	ì •ì˜:
$$ q(x_t | x_{t-1}) = \mathcal{N}(\sqrt{1 - \beta_t} x_{t-1}, \beta_t I) $$
	â€¢	ì ì°¨ì ìœ¼ë¡œ ë°ì´í„°ì— ë…¸ì´ì¦ˆë¥¼ ë”í•´ ì‹ í˜¸ë¥¼ ì™„ì „íˆ íŒŒê´´
	â€¢	$$ q(x_t | x_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I) $$

2.2 Reverse process (Sampling)
	â€¢	ëª©í‘œ: ë…¸ì´ì¦ˆê°€ ìˆëŠ” $x_t$ë¡œë¶€í„° $x_{t-1}$ì„ ìƒ˜í”Œë§í•´ $x_0$ì„ ë³µì›
	â€¢	$$ p_\theta(x_{t-1} | x_t) = \mathcal{N}(\mu_\theta(x_t, t), \Sigma_\theta(x_t, t)) $$

âœ… ForwardëŠ” ê³ ì •ëœ í™•ë¥  ë¶„í¬, ReverseëŠ” í•™ìŠµë˜ëŠ” neural networkë¡œ êµ¬ì„±ë¨

â¸»

ğŸ” 3. í•™ìŠµ ëª©ì  í•¨ìˆ˜ì™€ ìˆ˜ì‹ êµ¬ì¡°

3.1 Variational Lower Bound (VLB)
	â€¢	ì›ë˜ objective:

$$
\mathcal{L} = \mathbb{E}q\left[ D{KL}(q(x_T|x_0) \Vert p(x_T)) + \sum_{t>1} D_{KL}(q(x_{t-1}|x_t,x_0) \Vert p_\theta(x_{t-1}|x_t)) - \log p_\theta(x_0|x_1) \right]
$$
	â€¢	ê° í•­ì˜ ì˜ë¯¸:
	â€¢	$L_T$: priorì™€ noiseì˜ KL â†’ ìƒìˆ˜ë¡œ ë¬´ì‹œ ê°€ëŠ¥
	â€¢	$L_{t-1}$: ì—­ë°©í–¥ ì¡°ê±´ ë¶„í¬ì— ëŒ€í•œ matching
	â€¢	$L_0$: ìµœì¢… ë³µì› ì´ë¯¸ì§€ì— ëŒ€í•œ decoding likelihood
	â€¢	$x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon$
	â€¢	ëª©í‘œ: $\epsilon \sim \mathcal{N}(0, I)$ ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµ
	â€¢	ìƒˆë¡œìš´ loss:

$$
\mathcal{L}{\text{simple}} = \mathbb{E}{t, x_0, \epsilon} \left[ | \epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, t) |^2 \right]
$$

ğŸ¯ ìˆ˜ì‹ì ìœ¼ë¡œëŠ” ê°„ë‹¨í•˜ì§€ë§Œ, ì´ parameterizationì´ ì™œ ì„±ëŠ¥ì´ ì¢‹ì€ì§€ ê¹Šì´ ì´í•´ê°€ í•„ìš”í•¨

â¸»

ğŸš§ 4. ë‚´ê°€ ì–´ë ¤ì›€ì„ ê²ªì—ˆë˜ ê°œë…ë“¤

4.1 Îµ-prediction ë°©ì‹
	â€¢	$\mu_\theta$ ëŒ€ì‹  $\epsilon_\theta$ì„ ì˜ˆì¸¡í•˜ë©´:
	â€¢	ìˆ˜ì‹ì´ ë‹¨ìˆœí™”ë˜ê³  í•™ìŠµì´ ì•ˆì •ì ì„
	â€¢	denoising score matchingê³¼ ìœ ì‚¬í•œ í˜•íƒœë¡œ í•´ì„ ê°€ëŠ¥
	â€¢	Langevin dynamicsì™€ ìœ ì‚¬í•œ ìƒ˜í”Œë§ í•´ì„ì´ ê°€ëŠ¥

ë‚´ê°€ ì¼ë‹¤ë©´? â†’ í•™ìŠµì˜ íš¨ìœ¨ì„±ê³¼ ìƒ˜í”Œ í’ˆì§ˆì„ ìœ„í•´ ìˆ˜ì‹ì˜ ëª©ì í•¨ìˆ˜ë¥¼ ê°„ë‹¨íˆ ë°”ê¾¸ëŠ” ì‹œë„ë¥¼ í–ˆì„ ê²ƒ.

4.2 ì‹œê°„ ì„ë² ë”©ì˜ ì—­í• 
	â€¢	ì‹œê°„ $t$ì— ë”°ë¼ ë…¸ì´ì¦ˆ ë ˆë²¨ì´ ë‹¬ë¼ì§€ê¸° ë•Œë¬¸ì—, neural networkì— ë°˜ë“œì‹œ ì•Œë ¤ì¤˜ì•¼ í•¨
	â€¢	sinusoidal embedding (Transformer ë°©ì‹)ìœ¼ë¡œ $t$ë¥¼ feature spaceì— íˆ¬ì˜
	â€¢	U-Net ë‚´ë¶€ residual blockì— ì´ ì„ë² ë”©ì„ ì¶”ê°€í•´ ëª¨ë“  layerê°€ $t$ì— ì¡°ê±´í™”ë˜ë„ë¡ í•¨

ì–´ë ¤ì› ë˜ ë¶€ë¶„ì€: embeddingì´ ì–´ë–»ê²Œ skip connectionì„ ë”°ë¼ íë¥´ê³ , ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ feature mapê³¼ ì‘ìš©í•˜ëŠ”ì§€ ì§ê´€ì ìœ¼ë¡œ ë– ì˜¬ë¦¬ê¸° ì–´ë ¤ì› ìŒ

4.3 U-Net êµ¬ì¡°ì™€ skip connection
	â€¢	U-Netì€ pixelCNN++ ê¸°ë°˜ êµ¬ì¡°ë¥¼ ì±„íƒ
	â€¢	down blockì€ convolution + pooling â†’ ì´í•´ ì‰¬ì›€
	â€¢	up blockì€ transpose conv + skip connection
	â€¢	skip connectionì€ ê°™ì€ resolutionì˜ down block featureë¥¼ concat â†’ semantic ì •ë³´ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ë³µì›ì— ì‚¬ìš©

ë‚´ê°€ ê²ªì€ ì–´ë ¤ì›€: skip connectionì´ Îµ ì˜ˆì¸¡ì—ì„œ ì–´ë–¤ ì—­í• ì„ í•˜ëŠ”ì§€ ìˆ˜í•™ì ìœ¼ë¡œ ë³´ê¸´ í–ˆì§€ë§Œ, ì§ê´€ì  ì˜ë¯¸ë¥¼ íŒŒì•…í•˜ëŠ” ë° ì‹œê°„ì´ ê±¸ë ¸ìŒ

â¸»

ğŸ”„ 5. ìƒ˜í”Œë§ ì•Œê³ ë¦¬ì¦˜ ë¶„ì„ (Algorithm 2)

# Sampling pseudocode
x_T ~ N(0, I)
for t = T to 1:
    x_{t-1} = (1/âˆšalpha_t)(x_t - (1 - alpha_t)/âˆš(1 - \bar{alpha}_t) * Îµ_Î¸(x_t, t)) + Ïƒ_t * z

	â€¢	ë§ˆì¹˜ Langevin dynamicsì²˜ëŸ¼ ì‘ë™í•¨
	â€¢	í° íŠ¹ì§•ë¶€í„° ë¨¼ì € ë³µì›ë˜ê³  ì ì°¨ ë””í…Œì¼ì´ ì±„ì›Œì§ (coarse-to-fine)

Figure 6, 10ì„ í†µí•´ ì‹œê°ì ìœ¼ë¡œ í™•ì¸ ê°€ëŠ¥í•¨

â¸»

ğŸ§ª 6. ì‹¤í—˜ ìš”ì•½ (Section 4)
	â€¢	CIFAR10ì—ì„œ FID 3.17, Inception Score 9.46 (state-of-the-art)
	â€¢	ì‹¤í—˜ì ìœ¼ë¡œë„ $\epsilon$-predictionì´ $\mu$-predictionë³´ë‹¤ í’ˆì§ˆ ìš°ìˆ˜í•¨ì„ í™•ì¸
	â€¢	Reverse process varianceë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ê²½ìš° í•™ìŠµì´ ë¶ˆì•ˆì •í•´ì§

â¸»

ğŸ’¡ 7. ì´ ë…¼ë¬¸ì´ ë‚˜ì—ê²Œ ì¤€ ê´€ì 
	â€¢	ìˆ˜ì‹ì´ ë‹¨ìˆœí•´ ë³´ì—¬ë„, ê° êµ¬ì„±ìš”ì†ŒëŠ” ë§¤ìš° ì •êµí•œ ëª©ì ì„ ìœ„í•´ ì„¤ê³„ë¨
	â€¢	â€œì™œ ì´ëŸ° parameterizationì„ ì¼ì„ê¹Œ?â€œë¼ëŠ” ê´€ì ìœ¼ë¡œ ìˆ˜ì‹ì„ ë°”ë¼ë³´ë©´ ì´í•´ë„ê°€ í›¨ì”¬ ì˜¬ë¼ê°
	â€¢	ë‹¨ìˆœí•œ ì¬í˜„ì´ ì•„ë‹Œ ì§ê´€ê³¼ í•´ì„ì„ ê¸°ë°˜ìœ¼ë¡œ ì—°êµ¬ìì  ì‚¬ê³  í›ˆë ¨ì´ ê°€ëŠ¥í•¨

â¸»

ğŸ“š ì°¸ê³  ë¬¸í—Œ
	â€¢	[55] Yang Song & Stefano Ermon. Score-based generative models
	â€¢	[53] Jascha Sohl-Dickstein et al., Deep Unsupervised Learning Using Nonequilibrium Thermodynamics
	â€¢	[61] Pascal Vincent, Score matching and denoising autoencoders
