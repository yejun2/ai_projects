# Denoising Diffusion Probabilistic Models (DDPM)

> Jonathan Ho, Ajay Jain, Pieter Abbeel (UC Berkeley), NeurIPS 2020
> [arXiv:2006.11239](https://arxiv.org/abs/2006.11239)

---

## ğŸ“Œ 1. í•µì‹¬ ì•„ì´ë””ì–´ ìš”ì•½

* DDPMì€ **Markov Chain ê¸°ë°˜ latent variable model**ë¡œ, ë°ì´í„°ë¥¼ ì ì°¨ì ìœ¼ë¡œ ë…¸ì´ì¦ˆí™”í•œ í›„ ì—­ë°©í–¥ìœ¼ë¡œ ë³µì›í•˜ëŠ” ë°©ì‹.
* **Forward Process**: Gaussian noiseë¥¼ ì ì§„ì ìœ¼ë¡œ ì¶”ê°€.
* **Reverse Process**: í•™ìŠµëœ neural network (U-Net ê¸°ë°˜)ê°€ í•´ë‹¹ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•´ ì›ë³¸ì„ ë³µì›.
* **Training Objective**ëŠ” variational boundë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ì´ ê³¼ì •ì—ì„œ **denoising score matchingê³¼ Langevin dynamics**ê°€ ì—°ê²°ë¨.

### ğŸ“ DDPMì´ latent variable modelì¸ ì´ìœ 

> "ì§„ì§œ latent variable modelì¸ê°€?" ë¼ëŠ” ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ì„  êµ¬ì¡°ì™€ ìˆ˜ì‹ ê¸°ë°˜ í•´ì„ì´ í•„ìš”í•¨.

* ì¼ë°˜ì ì¸ latent variable modelì€ $x \sim p(x) = \int p(x|z)p(z)dz$ êµ¬ì¡°ë¥¼ ê°€ì§
* DDPMë„ $x_0 \sim p_\theta(x_0) = \int p_\theta(x_{0:T}) dx_{1:T}$ë¡œ í‘œí˜„ë¨
* ì—¬ê¸°ì„œ $x_1, ..., x_T$ëŠ” **dataì™€ ë™ì¼ ì°¨ì›ì˜ ì—°ì† latent**ë¡œ í•´ì„ë¨ â†’ trajectory ê¸°ë°˜ latent structure
* ForwardëŠ” encoder ì—­í•  (ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€), ReverseëŠ” decoder ì—­í•  (ë³µì›)
* $x_T \sim \mathcal{N}(0, I)$ ëŠ” prior ì—­í• ì„ ìˆ˜í–‰

ğŸ§© ê²°êµ­ DDPMì€:

* latentëŠ” í•˜ë‚˜ì˜ ë²¡í„°ê°€ ì•„ë‹ˆë¼ ì—°ì†ì ì¸ ë…¸ì´ì¦ˆ ìƒíƒœë“¤ $x_{1:T}$
* ì´ë“¤ì„ í†µí•´ $x_0$ë¥¼ ìƒì„±í•˜ë©°, variational inferenceë¥¼ í†µí•œ í•™ìŠµì„ ìˆ˜í–‰
* ë”°ë¼ì„œ **ëª…ë°±í•œ latent variable modelì˜ êµ¬ì¡°**ë¥¼ ë”°ë¦„

---

## ğŸ§  2. êµ¬ì¡° ê°œìš” (Forward & Reverse Process)

### 2.1 Forward process (Diffusion)

* ì •ì˜: $q(x_t | x_{t-1}) = \mathcal{N}(\sqrt{1 - \beta_t} x_{t-1}, \beta_t I)$
* ì ì°¨ì ìœ¼ë¡œ ë°ì´í„°ì— ë…¸ì´ì¦ˆë¥¼ ë”í•´ ì‹ í˜¸ë¥¼ ì™„ì „íˆ íŒŒê´´
* $q(x_t | x_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I)$

### 2.2 Reverse process (Sampling)

* ëª©í‘œ: ë…¸ì´ì¦ˆê°€ ìˆëŠ” $x_t$ë¡œë¶€í„° $x_{t-1}$ì„ ìƒ˜í”Œë§í•´ $x_0$ì„ ë³µì›
* $p_\theta(x_{t-1} | x_t) = \mathcal{N}(\mu_\theta(x_t, t), \Sigma_\theta(x_t, t))$

> âœ… ForwardëŠ” ê³ ì •ëœ í™•ë¥  ë¶„í¬, ReverseëŠ” í•™ìŠµë˜ëŠ” neural networkë¡œ êµ¬ì„±ë¨

---

## ğŸ” 3. í•™ìŠµ ëª©ì  í•¨ìˆ˜ì™€ ìˆ˜ì‹ êµ¬ì¡°

### 3.1 Variational Lower Bound (VLB)

* ì›ë˜ objective:

$$
\mathcal{L} = \mathbb{E}_q\left[ D_{KL}(q(x_T|x_0) || p(x_T)) + \sum_{t>1} D_{KL}(q(x_{t-1}|x_t,x_0) || p_\theta(x_{t-1}|x_t)) - \log p_\theta(x_0|x_1) \right]
$$

* ê° í•­ì˜ ì˜ë¯¸:

  * **$L_T$**: priorì™€ noiseì˜ KL â†’ ìƒìˆ˜ë¡œ ë¬´ì‹œ ê°€ëŠ¥
  * **$L_{t-1}$**: ì—­ë°©í–¥ ì¡°ê±´ ë¶„í¬ì— ëŒ€í•œ matching
  * **$L_0$**: ìµœì¢… ë³µì› ì´ë¯¸ì§€ì— ëŒ€í•œ decoding likelihood

- $x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon$
- ëª©í‘œ: $\epsilon \sim \mathcal{N}(0, I)$ ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµ
- ìƒˆë¡œìš´ loss:

$$
\mathcal{L}_{\text{simple}} = \mathbb{E}_{t, x_0, \epsilon} \left[ \| \epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, t) \|^2 \right]
$$

> ğŸ¯ ìˆ˜ì‹ì ìœ¼ë¡œëŠ” ê°„ë‹¨í•˜ì§€ë§Œ, ì´ parameterizationì´ ì™œ ì„±ëŠ¥ì´ ì¢‹ì€ì§€ ê¹Šì´ ì´í•´ê°€ í•„ìš”í•¨

---

## ğŸš§ 4. ë‚´ê°€ ì–´ë ¤ì›€ì„ ê²ªì—ˆë˜ ê°œë…ë“¤

* $\mu_\theta$ ëŒ€ì‹  $\epsilon_\theta$ì„ ì˜ˆì¸¡í•˜ë©´:

  * ìˆ˜ì‹ì´ ë‹¨ìˆœí™”ë˜ê³  í•™ìŠµì´ ì•ˆì •ì ì„
  * **denoising score matching**ê³¼ ìœ ì‚¬í•œ í˜•íƒœë¡œ í•´ì„ ê°€ëŠ¥
  * Langevin dynamicsì™€ ìœ ì‚¬í•œ ìƒ˜í”Œë§ í•´ì„ì´ ê°€ëŠ¥

> ë‚´ê°€ ì¼ë‹¤ë©´? â†’ í•™ìŠµì˜ íš¨ìœ¨ì„±ê³¼ ìƒ˜í”Œ í’ˆì§ˆì„ ìœ„í•´ ìˆ˜ì‹ì˜ ëª©ì í•¨ìˆ˜ë¥¼ ê°„ë‹¨íˆ ë°”ê¾¸ëŠ” ì‹œë„ë¥¼ í–ˆì„ ê²ƒ.

### 4.2 ì‹œê°„ ì„ë² ë”©ì˜ ì—­í• 

* ì‹œê°„ $t$ì— ë”°ë¼ ë…¸ì´ì¦ˆ ë ˆë²¨ì´ ë‹¬ë¼ì§€ê¸° ë•Œë¬¸ì—, neural networkì— ë°˜ë“œì‹œ ì•Œë ¤ì¤˜ì•¼ í•¨
* sinusoidal embedding (Transformer ë°©ì‹)ìœ¼ë¡œ $t$ë¥¼ feature spaceì— íˆ¬ì˜
* U-Net ë‚´ë¶€ residual blockì— ì´ ì„ë² ë”©ì„ ì¶”ê°€í•´ ëª¨ë“  layerê°€ $t$ì— ì¡°ê±´í™”ë˜ë„ë¡ í•¨

> ì–´ë ¤ì› ë˜ ë¶€ë¶„ì€: embeddingì´ ì–´ë–»ê²Œ skip connectionì„ ë”°ë¼ íë¥´ê³ , ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ feature mapê³¼ ì‘ìš©í•˜ëŠ”ì§€ ì§ê´€ì ìœ¼ë¡œ ë– ì˜¬ë¦¬ê¸° ì–´ë ¤ì› ìŒ

### 4.3 U-Net êµ¬ì¡°ì™€ skip connection

* U-Netì€ pixelCNN++ ê¸°ë°˜ êµ¬ì¡°ë¥¼ ì±„íƒ
* **down blockì€** convolution + pooling â†’ ì´í•´ ì‰¬ì›€
* **up blockì€** transpose conv + skip connection
* skip connectionì€ ê°™ì€ resolutionì˜ down block featureë¥¼ concat â†’ semantic ì •ë³´ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ë³µì›ì— ì‚¬ìš©

> ë‚´ê°€ ê²ªì€ ì–´ë ¤ì›€: skip connectionì´ Îµ ì˜ˆì¸¡ì—ì„œ ì–´ë–¤ ì—­í• ì„ í•˜ëŠ”ì§€ ìˆ˜í•™ì ìœ¼ë¡œ ë³´ê¸´ í–ˆì§€ë§Œ, ì§ê´€ì  ì˜ë¯¸ë¥¼ íŒŒì•…í•˜ëŠ” ë° ì‹œê°„ì´ ê±¸ë ¸ìŒ

---

## ğŸ”„ 5. ìƒ˜í”Œë§ ì•Œê³ ë¦¬ì¦˜ ë¶„ì„ (Algorithm 2)

```python
# Sampling pseudocode
x_T ~ N(0, I)
for t = T to 1:
    x_{t-1} = (1/âˆšalpha_t)(x_t - (1 - alpha_t)/âˆš(1 - \bar{alpha}_t) * Îµ_Î¸(x_t, t)) + Ïƒ_t * z
```

* ë§ˆì¹˜ **Langevin dynamics**ì²˜ëŸ¼ ì‘ë™í•¨
* í° íŠ¹ì§•ë¶€í„° ë¨¼ì € ë³µì›ë˜ê³  ì ì°¨ ë””í…Œì¼ì´ ì±„ì›Œì§ (coarse-to-fine)

> Figure 6, 10ì„ í†µí•´ ì‹œê°ì ìœ¼ë¡œ í™•ì¸ ê°€ëŠ¥í•¨

---

## ğŸ§ª 6. ì‹¤í—˜ ìš”ì•½ (Section 4)

* CIFAR10ì—ì„œ FID 3.17, Inception Score 9.46 (state-of-the-art)
* ì‹¤í—˜ì ìœ¼ë¡œë„ $\epsilon$-predictionì´ $\mu$-predictionë³´ë‹¤ í’ˆì§ˆ ìš°ìˆ˜í•¨ì„ í™•ì¸
* Reverse process varianceë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ê²½ìš° í•™ìŠµì´ ë¶ˆì•ˆì •í•´ì§

---

## ğŸ’¡ 7. ì´ ë…¼ë¬¸ì´ ë‚˜ì—ê²Œ ì¤€ ê´€ì 

* ìˆ˜ì‹ì´ ë‹¨ìˆœí•´ ë³´ì—¬ë„, ê° êµ¬ì„±ìš”ì†ŒëŠ” ë§¤ìš° ì •êµí•œ ëª©ì ì„ ìœ„í•´ ì„¤ê³„ë¨
* "ì™œ ì´ëŸ° parameterizationì„ ì¼ì„ê¹Œ?"ë¼ëŠ” ê´€ì ìœ¼ë¡œ ìˆ˜ì‹ì„ ë°”ë¼ë³´ë©´ ì´í•´ë„ê°€ í›¨ì”¬ ì˜¬ë¼ê°
* ë‹¨ìˆœí•œ ì¬í˜„ì´ ì•„ë‹Œ **ì§ê´€ê³¼ í•´ì„**ì„ ê¸°ë°˜ìœ¼ë¡œ ì—°êµ¬ìì  ì‚¬ê³  í›ˆë ¨ì´ ê°€ëŠ¥í•¨

---

## ğŸ“š ì°¸ê³  ë¬¸í—Œ

* \[55] Yang Song & Stefano Ermon. Score-based generative models
* \[53] Jascha Sohl-Dickstein et al., Deep Unsupervised Learning Using Nonequilibrium Thermodynamics
* \[61] Pascal Vincent, Score matching and denoising autoencoders
